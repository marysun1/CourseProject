---
title: "Course Project"
subtitle: Mary Sun
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
<br>
Description: This document is my peer assessment write-up for the Course Project in the Practical Machine Learning Coursera course produced by Johns Hopkins University. It was built using the knitr functions in RStudio and published to HTML format. 
<br>

## 1. Background
The goal of this project is to predict the manner in which 6 participants performed an exercise based on data from accelerometers on the belt, forearm, arm, and dumbbell. All data was sourced from <http://groupware.les.inf.puc-rio.br/har> and the outcome variable (manner of exercise) was captured as the “classe” variable in the provided training set. The machine learning algorithm that is described was applied to  20 test cases provided in the test data. This document describes how the model was built, how cross validation was used, the expected out of sample error, and the rationale for the prediction method I chose.

## 2. Data exploration
This data was generated by six healthy male participants, aged 20-28, who performed one set of 10 repetitions of the Unilateral Dumbbell Biceps Curl in five different ways: according to specification (Class A), with elbows thrown to front (Class B), lifting the dumbbell halfway (Class C), lowering the dumbbell halfway (Class D), and with hips thrown to the front (Class E). Classes B-E are meant to correspond to common mistakes while performing this exercise. Participants did not have significant weight lifting experienced and were supervised by an experienced weight lifter to ensure compliance with the specifications. The dummbbell was relatively light at 1.25kg, to ensure safe and controlled movement.

### 2A. Environment preparation
The library necessary for the following analysis are loaded
```{r}
## Libraries 
library(knitr)
library(caret)
library(rpart)
library(rpart.plot)
library(rattle)
library(randomForest)
library(corrplot)
set.seed(12345)
```

### 2B. Data loading
Data from the specified training and testing CSV files are downloaded and stored as objects in R.
```{r}
## Data download
UrlTrain <- "http://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
UrlTest  <- "http://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
training <- data.frame(read.csv(url(UrlTrain)))
testing  <- data.frame(read.csv(url(UrlTest)))
```

### 2C. Data partitioning
The training data is partitioned into two parts to create a Training set (80% of data) for the model selection process and a Test set (20% of data) for use in validation. Note that the testing data is not changed during this analysis and is only used for results generation. 
```{r}
# Partition training data
inTrain  <- createDataPartition(training$classe, p=0.8, list=FALSE)
TrainSet <- training[inTrain, ]
TestSet  <- training[-inTrain, ]

#Exploring dimensions of partitioned data
dim(TrainSet)
dim(TestSet)
```
As we can see above, both datasets have 160 variables. 

### 2D. Data cleaning
First, we remove variables with close to zero variance. It is unlikely that these variables will contribute significantly to the model, since they do not vary themselves and therefore will not explain variance in our outcome of interest ("classe").
```{r}
## Cleaning: remove variables with close to zero variance
NZV <- nearZeroVar(TrainSet)
TrainSet <- TrainSet[, -NZV]
TestSet  <- TestSet[, -NZV]
dim(TrainSet)
dim(TestSet)
```
This step reduces the number of variables from 160 to 101.
<br>
Second, we remove variables with values that are, on average, missing in 95% of cases.
```{r}
# Cleaning: remove variables that are majority NA
AllNA    <- sapply(TrainSet, function(x) mean(is.na(x))) > 0.95
TrainSet <- TrainSet[, AllNA==FALSE]
TestSet  <- TestSet[, AllNA==FALSE]
dim(TrainSet)
dim(TestSet)
```
This step reduces the number of variables from 101 to 59.
<br>
Third, we remove variables that are used for identification only and thus not contributory to our model prediction.
```{r}
## Cleaning: remove identification only variables (columns 1 to 5)
TrainSet <- TrainSet[, -(1:5)]
TestSet  <- TestSet[, -(1:5)]
dim(TrainSet)
dim(TestSet)
```
This step reduces the number of variables from 59 to 54.

### 2E. Correlation analysis among variables
Before proceeding to modeling, a correlation analysis is performed to see if there are many variables that are highly correlated. If so, a PCA (Principal Components Analysis) will be performed to reduce the number of variables and extract their useful features.
```{r}
## Correlation analysis among remaining variables
corMatrix <- cor(TrainSet[, -54])
corrplot(corMatrix, order = "FPC", method = "color", type = "upper", 
         tl.cex = 0.8, tl.col = rgb(0, 0, 0))
```
The above plot can be interpreted using a heatmap legend, where darker colored areas indicate higher degrees of positive (blue) or negative (red) correlation as measured on the right hand side vertical axis. While there are dark areas, they are not numerous enough to warrant a PCA. Therefore, we proceed to the modeling step.

## 3. Modeling
This analysis evaluates three predictive techniques to model the regressions in the training dataset: random forests, decision trees, and the generalized boosted model (gbm). Each technique is evaluated below and a confusion matrix is included with each analysis to visualized relative accuracy.

### 3A. Modeling: Random Forests
```{r}
## Prediction model: random forest
set.seed(12345)
controlRF <- trainControl(method="cv", number=3, verboseIter=FALSE)
modFitRandForest <- train(classe ~ ., data=TrainSet, method="rf", trControl=controlRF)
modFitRandForest$finalModel

## Prediction on test data
predictRandForest <- predict(modFitRandForest, newdata=TestSet)
confMatRandForest <- confusionMatrix(table(predictRandForest, TestSet$classe))
confMatRandForest

## Plot results
plot(confMatRandForest$table, col = confMatRandForest$byClass, main = paste("Random Forest - Accuracy =", round(confMatRandForest$overall['Accuracy'], 4)))
```

### 3B. Modeling: Decision Trees
```{r}
## Prediction model: decision trees
set.seed(12345)
modFitDecTree <- rpart(classe ~ ., data=TrainSet, method="class")
fancyRpartPlot(modFitDecTree)

## Prediction on test data
predictDecTree <- predict(modFitDecTree, newdata=TestSet, type="class")
confMatDecTree <- confusionMatrix(table(predictDecTree, TestSet$classe))
confMatDecTree

## Plot results
plot(confMatDecTree$table, col = confMatDecTree$byClass, 
     main = paste("Decision Tree - Accuracy =",
                  round(confMatDecTree$overall['Accuracy'], 4)))
```

### 3B. General Boosted Model (GBM)
```{r}
## Prediction model: general boosted model
set.seed(12345)
controlGBM <- trainControl(method = "repeatedcv", number = 5, repeats = 1)
modFitGBM  <- train(classe ~ ., data=TrainSet, method = "gbm",
                    trControl = controlGBM, verbose = FALSE)
modFitGBM$finalModel

## Prediction on test data
predictGBM <- predict(modFitGBM, newdata=TestSet)
confMatGBM <- confusionMatrix(table(predictGBM, TestSet$classe))
confMatGBM

## Plot results
plot(confMatGBM$table, col = confMatGBM$byClass, 
     main = paste("GBM - Accuracy =", round(confMatGBM$overall['Accuracy'], 4)))
```

## 4. Model Selection
From the analyses above, we see that the accuracy values for each of the 3 models are:
+ Random forests: 0.9999
+ Decision trees: 0.7285
+ Generalized boosting: 0.9895
<br>
Therefore, we choose the random forests model to apply to the test data of 20 individuals since it has the highest accuracy value.
```{r}
## Random forest method has highest accuracy, apply it to testing dataset
predictOnTest <- predict(modFitRandForest, newdata=testing)
predictOnTest
```
